import json
import sys
import re
from pathlib import Path

# --- æ ‡å‡†åŒ–é…ç½® ---
# è„šæœ¬ä»C++æ§åˆ¶å™¨æ¥æ”¶å·¥ä½œç›®å½•ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°
SOURCE_DIR = Path(sys.argv[1]) if len(sys.argv) > 1 else Path(".") 
OUTPUT_DIR = Path("google_ai_studio_markdown_output")

# (sanitize_filename, get_unique_filepath, convert_google_ai_chat_to_markdown å‡½æ•°ä¸å˜)
def sanitize_filename(filename: str) -> str:
    """å¢å¼ºç‰ˆæ–‡ä»¶åæ¸…ç†å‡½æ•°ã€‚"""
    if not filename or not filename.strip(): return "æœªå‘½åå¯¹è¯"
    filename = filename.replace('\n', ' ').replace('\r', ' ')
    filename = re.sub(r'[\\/*?:"<>|]', "", filename)
    filename = re.sub(r'\s+', '_', filename)
    filename = filename[:100]; filename = filename.strip('_')
    return filename or "ç‰¹æ®Šå­—ç¬¦å¯¹è¯"

def get_unique_filepath(output_dir: Path, filename: str) -> Path:
    """ç¡®ä¿ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„æ˜¯å”¯ä¸€çš„ï¼Œé˜²æ­¢è¦†ç›–ã€‚"""
    base_path = output_dir / (filename + ".md")
    if not base_path.exists(): return base_path
    counter = 1
    while True:
        new_path = output_dir / f"{filename}({counter}).md"
        if not new_path.exists(): return new_path
        counter += 1

def convert_google_ai_chat_to_markdown(conversation_data: dict, output_md_path: Path) -> bool:
    """ã€æ‚¨çš„åŸå§‹æ ¸å¿ƒé€»è¾‘ã€‘ä»å•ä¸ªGoogle AIå¯¹è¯æ•°æ®ä¸­æå–å¯¹è¯ã€‚"""
    if 'chunkedPrompt' not in conversation_data or 'chunks' not in conversation_data['chunkedPrompt']: return False
    all_chunks = conversation_data['chunkedPrompt']['chunks']
    conversation_chunks = [c for c in all_chunks if not c.get('isThought', False)]
    if not conversation_chunks: return False
    has_content = False
    with open(output_md_path, 'w', encoding='utf-8') as md_file:
        for chunk in conversation_chunks:
            role, text = chunk.get('role'), chunk.get('text', '').strip()
            if role == 'user':
                if text: md_file.write(f"# ç”¨æˆ·: {text}\n\n---\n\n"); has_content = True
                elif 'driveImage' in chunk: md_file.write(f"# ç”¨æˆ·: (ä¸Šä¼ äº†ä¸€å¼ å›¾ç‰‡)\n\n---\n\n"); has_content = True
            elif role == 'model' and text: md_file.write(f"## å›ç­”\n\n{text}\n\n"); has_content = True
    return has_content

def process_single_file(input_path: Path):
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶å¹¶è¿”å›ç»“æœ (æˆåŠŸ, è·³è¿‡, æ–‡ä»¶é”™è¯¯)ã€‚
    è¿™ä¸ªå‡½æ•°ä¿æŒâ€œå®‰é™â€ï¼Œåªè¿”å›çŠ¶æ€ç ã€‚
    """
    try:
        with open(input_path, 'r', encoding='utf-8') as f: conversation_data = json.load(f)
    except Exception:
        return 0, 0, 1
        
    if convert_google_ai_chat_to_markdown(conversation_data, input_path):
        # æˆåŠŸï¼Œé‡å‘½åæ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
        title = input_path.stem
        safe_filename = sanitize_filename(title)
        md_path = get_unique_filepath(OUTPUT_DIR, safe_filename)
        # è¿™æ˜¯ä¸€ä¸ªå°æŠ€å·§ï¼Œå› ä¸ºconvertå‡½æ•°å·²ç»å†™å…¥äº†åŸè·¯å¾„ï¼Œæˆ‘ä»¬ç›´æ¥ç§»åŠ¨å®ƒ
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            OUTPUT_DIR.mkdir(exist_ok=True)
            input_path.rename(md_path)
            return 1, 0, 0
        except Exception:
            return 0, 0, 1
    else:
        # å¯¹è¯ä¸ºç©ºæˆ–ç»“æ„ä¸ç¬¦
        return 0, 1, 0

def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼Œç°åœ¨ä¼šè‡ªå·±è®¡ç®—æ€»æ–‡ä»¶æ•°å¹¶æŠ¥å‘Šè¿›åº¦ã€‚
    """
    print(f"--- Google AI/Chunks æ ¼å¼è½¬æ¢å™¨ ---")
    print(f"æ•°æ®æº: '{SOURCE_DIR.resolve()}'")
    if not SOURCE_DIR.is_dir():
        print(f"é”™è¯¯: æºç›®å½• '{SOURCE_DIR}' ä¸å­˜åœ¨ã€‚")
        sys.exit(1)
        
    try:
        OUTPUT_DIR.mkdir(exist_ok=True)
    except PermissionError:
        print(f"ä¸¥é‡é”™è¯¯: æ— æ³•åˆ›å»ºæ–‡ä»¶å¤¹ '{OUTPUT_DIR}'ã€‚è¯·æ£€æŸ¥æƒé™ã€‚")
        sys.exit(1)
    
    # --- ã€æ ¸å¿ƒä¿®æ­£ã€‘åœ¨å¼€å§‹å‰ï¼Œå…ˆè·å–æ–‡ä»¶åˆ—è¡¨å¹¶è®¡ç®—æ€»æ•° ---
    json_files_to_process = sorted(list(SOURCE_DIR.glob('*.json')))
    total_files = len(json_files_to_process)

    if total_files == 0:
        print("ä¿¡æ¯: åœ¨ 'conversation' æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .json æ–‡ä»¶ã€‚")
        sys.exit(0)
    
    print(f"åˆ†æå®Œæˆï¼æ‰¾åˆ° {total_files} ä¸ªJSONæ–‡ä»¶ï¼Œå‡†å¤‡å¼€å§‹è½¬æ¢...")
    print("\n--- é˜¶æ®µ 2: å¼€å§‹æ­£å¼è½¬æ¢ ---")
    
    total_success, total_skipped, total_file_errors = 0, 0, 0
    
    # --- åœ¨å¾ªç¯ä¸­æŠ¥å‘Šæ­£ç¡®çš„è¿›åº¦ ---
    for i, filepath in enumerate(json_files_to_process):
        # åœ¨è¿™é‡Œæ‰“å°ç»Ÿä¸€çš„ã€æœ‰æ„ä¹‰çš„è¿›åº¦ï¼
        print(f"\n--- [{i+1}/{total_files}] å¤„ç†æ–‡ä»¶: {filepath.name} ---")
        
        # ä¸ºäº†é¿å…åœ¨åŸæ–‡ä»¶å¤¹ç•™ä¸‹åƒåœ¾æ–‡ä»¶ï¼Œæˆ‘ä»¬å…ˆè¯»å–å†…å®¹
        try:
            with filepath.open('r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception:
            print("  -> âŒ é”™è¯¯: è¯»å–æ–‡ä»¶å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚")
            total_file_errors += 1
            continue

        title = filepath.stem
        safe_filename = sanitize_filename(title)
        md_path = get_unique_filepath(OUTPUT_DIR, safe_filename)

        if convert_google_ai_chat_to_markdown(data, md_path):
             print(f"  -> âœ… æˆåŠŸç”Ÿæˆ: {md_path.name}")
             total_success += 1
        else:
             print(f"  -> ğŸŸ¡ è­¦å‘Š: å¯¹è¯ä¸ºç©ºæˆ–ç»“æ„ä¸å®Œæ•´ï¼Œå·²è·³è¿‡ã€‚")
             total_skipped += 1

    # (æœ€ç»ˆæŠ¥å‘Šéƒ¨åˆ†ä¸å˜)
    print("\n" + "="*40)
    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼ ğŸ‰")
    print(f"æ€»è®¡æˆåŠŸç”Ÿæˆ: {total_success} ä¸ªMarkdownæ–‡æ¡£")
    if total_skipped > 0:
        print(f"æ€»è®¡è·³è¿‡(ç©ºæˆ–æŸåçš„å¯¹è¯): {total_skipped} ä¸ª")
    if total_file_errors > 0:
        print(f"å¤„ç†å¤±è´¥çš„æ–‡ä»¶æ•°: {total_file_errors} ä¸ª")
    print(f"æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜è‡³ '{OUTPUT_DIR}' æ–‡ä»¶å¤¹ã€‚")

if __name__ == "__main__":
    main()